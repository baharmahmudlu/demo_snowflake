# CREATE
create database DB_NAME;
create schema SCHEMA_NAME;
create role ROLE_NAME;

# CREATE WAREHOUSE
create warehouse WH_NAME 
with 
warehouse_size = 'SIZE_OF_WH'  -- 'XSMALL' 
warehouse_type =  'TYPE_OF_WH' -- 'STANDARD' 
auto_suspend = 600 --600 seconds/10 mins
auto_resume = TRUE;

# USE
use database DB_NAME;
use schema SCHEMA_NAME;
use role ROLE_NAME;

# DROP
drop database DB_NAME;
drop schema SCHEMA_NAME;
drop role ROLE_NAME;

# SHOW
show schemas;
show tables;
show schemas in account;
show tables in account;
show resource monitors in account;
show shares in account;

# ALTER DATABASE NAME
alter database DB_NAME rename to NEW_DB_NAME;

# ALTER TABLE 
alter table TABLE_NAME add column COL_NAME varchar(<number_of_characters>);

# GRANT ROLE TO DB
grant imported privileges on database DB_NAME to role ROLE_NAME;
grant all privileges on schema SCHEMA_NAME to role ROLE_NAME;

# CREATE TABLE
create or replace table TABLE_NAME (
    COL1 data_type,
    COL2 data_type,
);

# CREATE TABLE WITH AUTOINCREMENT to generate a UID for each new row
create or replace table TABLE_NAME
( ID number autoincrement
 , COL2 varchar(50)
 , COL3 number(4,0)
);

# INSERT INTO TABLE
insert into TABLE_NAME values (VAL1, VAL2);

# UPDATE VALUE IN TABLE
update TABLE_NAME set COL = VAL where CONDITION

# REMOVE ROW FROM TABLE
delete from TABLE_NAME
where CONDITION;

# CLEAN WHOLE TABLE
truncate table TABLE_NAME;

# CREATE FILE FORMAT TO STORE DATA FROM STAGE
create file format DB_NAME.SCHEMA_NAME.FILE_FORMAT_NAME 
    type = 'CSV'  # csv is used for any flat file (tsv, pipe-separated, etc)
    field_delimiter = '|'  # pipes as column separators
    skip_header = 1  # one header row to skip
    ;

create file format DB_NAME.SCHEMA_NAME.FILE_FORMAT_NAME 
    type = 'CSV'  # csv for comma separated files
    field_delimiter = ','  # commas as column separators
    skip_header = 1 --one header row  
    FIELD_OPTIONALLY_ENCLOSED_BY = '"'  # this means that some values will be wrapped in double-quotes bc they have commas in them
    ;

create file format DB_NAME.SCHEMA_NAME.FILE_FORMAT_NAME
    type = 'CSV'
    field_delimiter = '\t'  # tab as column separators
    skip_header = 1
    ;

//Create File Format for JSON Data 
create or replace file format DB_NAME.SCHEMA_NAME.FILE_FORMAT_NAME
type = 'JSON' 
compression = 'AUTO' 
strip_outer_array = TRUE; 

//Create file format to allow the 3 column file to be loaded into an 18 column table
// By parsing the header, Snowflake can infer the column names
create or replace file format DB_NAME.SCHEMA_NAME.FILE_FORMAT_NAME
type = 'CSV' 
field_delimiter = ',' 
record_delimiter = '\n' 
field_optionally_enclosed_by = '"'
trim_space = TRUE
error_on_column_count_mismatch = FALSE
parse_header = TRUE;

# SELECT FROM STAGE DIRECTLY
# before selecting we need to add file to stage (database -> schema -> create btn -> stage btn -> snowflake managed btn -> upload)
select $1, $2, $3
from @DB_NAME.SCHEMA_NAME.STAGE_NAME/FILE_NAME
(file_format => FILE_FORMAT_NAME_PATH );

# COPY INTO TABLE FROM STAGE 
copy into TABLE_NAME
from @FILE_PATH
files = ( 'STAGE_FILE_NAME' )
file_format = ( format_name=FILE_FORMAT_NAME_PATH );

//copy into a table by using file format that allows the 3 column file to be loaded into an 18 column table
copy into TABLE_NAME
from @FILE_PATH
file_format = ( format_name=FILE_FORMAT_NAME_PATH )
match_by_column_name='CASE_INSENSITIVE';

# SEQUENCE 
# we can create a sequence as a unique identifier to use when we insert into a table
create or replace sequence DB_NAME.SCHEMA_NAME.SEQUENCE_NAME
start = START_NUMBER 
increment = INCREMENT_NUMBER
order
comment = 'YOUR_COMMENT_HERE';

// sample to use
insert into TABLE_NAME(ID_COL,COL2, COL3) 
values
(seq_author_uid.nextval, 'Laura', 'K')
,(seq_author_uid.nextval, 'Jan', '')
,(seq_author_uid.nextval, 'Jennifer', '')
,(seq_author_uid.nextval, 'Kathleen', '');

# CREATE VARIABLE AND SELECT
set VAR_NAME = 'VAR_VALUE';

select $VAR_NAME;

